name: Run All Scrapers

on:
  schedule:
    - cron: '0 3 * * *' # Runs daily at 3 AM UTC
  workflow_dispatch: # Allows manual triggering

jobs:
  run-scrapers:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      HIBP_API_KEY: ${{ secrets.HIBP_API_KEY }}
      APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
      APIFY_TEXAS_BREACH_ACTOR_ID: ${{ secrets.APIFY_TEXAS_BREACH_ACTOR_ID }}
      PYTHONUNBUFFERED: "1" # Ensures print statements appear in the GitHub Actions logs immediately

      # California AG scraper configuration for GitHub Actions
      CA_AG_FILTER_FROM_DATE: "2025-05-21" # One week back for reliable testing
      CA_AG_PROCESSING_MODE: "FULL" # Complete PDF analysis with "What information was involved?" extraction

      # Washington AG scraper configuration for GitHub Actions
      WA_AG_FILTER_FROM_DATE: "2024-01-20" # One week back for reliable testing

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install -r requirements.txt

      # --- Government & State AG Scrapers ---


      - name: Run California AG Scraper
        run: python scrapers/fetch_california_ag.py

     
      # --- Final Step ---
      - name: All scrapers finished
        run: echo "All scraper scripts have been executed."
