name: Run All Scrapers (Parallel)

on:
  schedule:
    - cron: '*/30 * * * *' # Runs every 30 minutes
  workflow_dispatch: # Allows manual triggering
    inputs:
      run_government:
        description: 'Run Government & Federal Scrapers'
        required: false
        default: true
        type: boolean
      run_state_ag_1:
        description: 'Run State AG Group 1 (DE, CA, WA, HI)'
        required: false
        default: true
        type: boolean
      run_state_ag_2:
        description: 'Run State AG Group 2 (IN, IA, ME)'
        required: false
        default: true
        type: boolean
      run_state_ag_3:
        description: 'Run State AG Group 3 (MA, MT, NH, NJ)'
        required: false
        default: true
        type: boolean
      run_state_ag_4:
        description: 'Run State AG Group 4 (ND, OK, VT, WI, TX)'
        required: false
        default: true
        type: boolean
      run_problematic:
        description: 'Run Problematic Scrapers (MD)'
        required: false
        default: true
        type: boolean
      scraping_frequency:
        description: 'Set scraping frequency (for future scheduling)'
        required: false
        default: 'daily'
        type: choice
        options:
          - 'daily'
          - 'every-6-hours'
          - 'every-12-hours'
          - 'twice-daily'

jobs:
  # ============================================================================
  # PRE-SCRAPING SNAPSHOT - Capture database state before scraping
  # ============================================================================

  pre-scraping-snapshot:
    name: "ðŸ“¸ Pre-Scraping Database Snapshot"
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      PYTHONUNBUFFERED: "1"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Take database snapshot
        run: python scrapers/database_change_tracker.py --snapshot
      - name: Upload snapshot
        uses: actions/upload-artifact@v4
        with:
          name: database-snapshot
          path: /tmp/scraper_snapshot.json
          retention-days: 1

  # ============================================================================
  # PARALLEL SCRAPER GROUPS - Run simultaneously for faster execution
  # ============================================================================

  government-scrapers:
    name: "Government & Federal Scrapers"
    runs-on: ubuntu-latest
    needs: pre-scraping-snapshot
    if: ${{ github.event.inputs.run_government != 'false' }}
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      HIBP_API_KEY: ${{ secrets.HIBP_API_KEY }}
      PYTHONUNBUFFERED: "1"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run SEC EDGAR 8-K Scraper
        run: python scrapers/fetch_sec_edgar_8k.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run HHS OCR Scraper
        run: python scrapers/fetch_hhs_ocr.py

  state-ag-group-1:
    name: "State AG Group 1 (DE, CA, WA, HI)"
    runs-on: ubuntu-latest
    needs: pre-scraping-snapshot
    if: ${{ github.event.inputs.run_state_ag_1 != 'false' }}
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      PYTHONUNBUFFERED: "1"
      # California AG configuration
      CA_AG_FILTER_FROM_DATE: "2025-05-21"
      CA_AG_PROCESSING_MODE: "FULL"
      # Washington AG configuration
      WA_AG_FILTER_FROM_DATE: "2025-01-20"
      # Hawaii AG configuration
      HI_AG_FILTER_FROM_DATE: "2025-01-21"
      HI_AG_PROCESSING_MODE: "FULL"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run Delaware AG Scraper
        run: python scrapers/fetch_delaware_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run California AG Scraper
        run: python scrapers/fetch_california_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run Washington AG Scraper
        run: python scrapers/fetch_washington_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run Hawaii AG Scraper
        run: python scrapers/fetch_hi_ag.py

  state-ag-group-2:
    name: "State AG Group 2 (IN, IA, ME)"
    runs-on: ubuntu-latest
    needs: pre-scraping-snapshot
    if: ${{ github.event.inputs.run_state_ag_2 != 'false' }}
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      PYTHONUNBUFFERED: "1"
      # Indiana AG configuration
      IN_AG_FILTER_FROM_DATE: "2025-01-20"
      IN_AG_PROCESSING_MODE: "ENHANCED"
      # Iowa AG configuration
      IA_AG_FILTER_FROM_DATE: "2025-01-20"
      IA_AG_PROCESSING_MODE: "ENHANCED"
      # Maine AG configuration
      ME_AG_FILTER_FROM_DATE: "2025-01-01"
      ME_AG_PROCESSING_MODE: "ENHANCED"
      ME_AG_MAX_PAGES: "2"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run Indiana AG Scraper
        run: python scrapers/fetch_in_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run Iowa AG Scraper
        run: python scrapers/fetch_ia_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run Maine AG Scraper
        run: python scrapers/fetch_me_ag.py

  state-ag-group-3:
    name: "State AG Group 3 (MA, MT, NH, NJ)"
    runs-on: ubuntu-latest
    needs: pre-scraping-snapshot
    if: ${{ github.event.inputs.run_state_ag_3 != 'false' }}
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      PYTHONUNBUFFERED: "1"
      # Massachusetts AG configuration
      MA_AG_FILTER_DAYS_BACK: "7"
      MA_AG_PROCESSING_MODE: "ENHANCED"
      MA_AG_FORCE_PROCESS: "true"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run Massachusetts AG Scraper
        run: python scrapers/fetch_ma_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run Montana AG Scraper
        run: python scrapers/fetch_mt_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run New Hampshire AG Scraper
        run: python scrapers/fetch_nh_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run New Jersey Cybersecurity Scraper
        run: python scrapers/fetch_nj_ag.py

  state-ag-group-4:
    name: "State AG Group 4 (ND, OK, VT, WI, TX)"
    runs-on: ubuntu-latest
    needs: pre-scraping-snapshot
    if: ${{ github.event.inputs.run_state_ag_4 != 'false' }}
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
      APIFY_TEXAS_BREACH_ACTOR_ID: ${{ secrets.APIFY_TEXAS_BREACH_ACTOR_ID }}
      PYTHONUNBUFFERED: "1"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run North Dakota AG Scraper
        run: python scrapers/fetch_nd_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run Oklahoma Cybersecurity Scraper
        run: python scrapers/fetch_ok_cyber.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run Vermont AG Scraper
        run: python scrapers/fetch_vt_ag.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run Wisconsin DATCP Scraper
        run: python scrapers/fetch_wi_datcp.py
      - name: Wait between scrapers
        run: sleep 10
      - name: Run Texas AG Scraper
        run: python scrapers/fetch_texas_ag.py

  # ============================================================================
  # NOTE: RSS & API scrapers have been moved to a separate workflow
  # See: .github/workflows/rss-api-scrapers.yml
  # This separation prevents RSS/API failures from affecting state portal scrapers
  # ============================================================================

  # ============================================================================
  # PROBLEMATIC SCRAPERS - Sites with known issues (run separately for monitoring)
  # ============================================================================

  problematic-scrapers:
    name: "Problematic Scrapers (MD)"
    runs-on: ubuntu-latest
    needs: pre-scraping-snapshot
    if: ${{ github.event.inputs.run_problematic != 'false' }}
    continue-on-error: true # Don't fail the workflow if these scrapers fail
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      PYTHONUNBUFFERED: "1"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run Maryland AG Scraper (Known Issues)
        run: |
          echo "âš ï¸ Running Maryland AG scraper - known to have website issues"
          python scrapers/fetch_md_ag.py || echo "âŒ Maryland AG scraper failed as expected"

  # ============================================================================
  # SUMMARY JOB - Waits for all parallel jobs to complete
  # ============================================================================

  summary:
    name: "ðŸ“Š Scraping Summary & Database Changes"
    runs-on: ubuntu-latest
    needs: [pre-scraping-snapshot, government-scrapers, state-ag-group-1, state-ag-group-2, state-ag-group-3, state-ag-group-4, problematic-scrapers]
    if: always() # Run even if some jobs fail
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      PYTHONUNBUFFERED: "1"
    outputs:
      new_items: ${{ steps.database-report.outputs.new_items }}
      new_breaches: ${{ steps.database-report.outputs.new_breaches }}
      new_news: ${{ steps.database-report.outputs.new_news }}
      new_affected: ${{ steps.database-report.outputs.new_affected }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Download snapshot
        uses: actions/download-artifact@v4
        with:
          name: database-snapshot
          path: /tmp/
      - name: Generate database change report
        id: database-report
        run: python scrapers/database_change_tracker.py --report
      - name: Show today's daily activity context
        run: |
          echo ""
          echo "ðŸ“… TODAY'S DAILY CONTEXT (1am to now):"
          python scrapers/daily_change_tracker.py --today
      - name: Check job results
        run: |
          echo "=== PARALLEL SCRAPING SUMMARY ==="
          echo "Government & Federal: ${{ needs.government-scrapers.result }}"
          echo "State AG Group 1 (DE,CA,WA,HI): ${{ needs.state-ag-group-1.result }}"
          echo "State AG Group 2 (IN,IA,ME): ${{ needs.state-ag-group-2.result }}"
          echo "State AG Group 3 (MA,MT,NH,NJ): ${{ needs.state-ag-group-3.result }}"
          echo "State AG Group 4 (ND,OK,VT,WI,TX): ${{ needs.state-ag-group-4.result }}"
          echo "Problematic Scrapers (MD): ${{ needs.problematic-scrapers.result }}"
          echo "================================="
          echo "â„¹ï¸  Note: RSS & API scrapers now run in separate workflow"

          # Count successful jobs (excluding problematic scrapers from main count)
          success_count=0
          total_count=5

          [[ "${{ needs.government-scrapers.result }}" == "success" ]] && ((success_count++))
          [[ "${{ needs.state-ag-group-1.result }}" == "success" ]] && ((success_count++))
          [[ "${{ needs.state-ag-group-2.result }}" == "success" ]] && ((success_count++))
          [[ "${{ needs.state-ag-group-3.result }}" == "success" ]] && ((success_count++))
          [[ "${{ needs.state-ag-group-4.result }}" == "success" ]] && ((success_count++))

          echo "âœ… Successful core groups: $success_count/$total_count"

          # Report on problematic scrapers separately
          if [[ "${{ needs.problematic-scrapers.result }}" == "success" ]]; then
            echo "ðŸ”§ Problematic scrapers: Unexpectedly succeeded!"
          else
            echo "âš ï¸ Problematic scrapers: Failed as expected (Maryland AG has known website issues)"
          fi

          if [ $success_count -eq $total_count ]; then
            echo "ðŸŽ‰ All core scraper groups completed successfully!"
          elif [ $success_count -gt 0 ]; then
            echo "âš ï¸ Some core scraper groups failed, but $success_count groups succeeded"
          else
            echo "âŒ All core scraper groups failed"
            exit 1
          fi

          echo ""
          echo "ðŸ“Š DATABASE CHANGES SUMMARY:"
          echo "   Items Change: ${{ steps.database-report.outputs.new_items }}"
          echo "   Breaches Change: ${{ steps.database-report.outputs.new_breaches }}"
          echo "   News Change: ${{ steps.database-report.outputs.new_news }}"
          echo "   People Affected Change: ${{ steps.database-report.outputs.new_affected }}"

          # Interpret the results
          NEW_ITEMS="${{ steps.database-report.outputs.new_items }}"
          NEW_BREACHES="${{ steps.database-report.outputs.new_breaches }}"

          # Handle empty values by defaulting to 0
          NEW_ITEMS=${NEW_ITEMS:-0}
          NEW_BREACHES=${NEW_BREACHES:-0}

          if [ "$NEW_ITEMS" -lt 0 ]; then
            echo "â„¹ï¸  Note: Negative values indicate database cleanup (duplicate removal)"
          elif [ "$NEW_ITEMS" -eq 0 ]; then
            echo "â„¹ï¸  No new items discovered in this run"
          else
            echo "âœ… $NEW_ITEMS new items discovered!"
          fi

          # Always exit successfully if core groups succeeded
          echo "ðŸŽ‰ Workflow completed successfully!"

  # ============================================================================
  # EMAIL ALERTS JOB - Send email notifications to subscribers
  # ============================================================================

  email-alerts:
    name: "ðŸ“§ Email Alert Notifications"
    runs-on: ubuntu-latest
    needs: summary
    if: ${{ always() && needs.summary.outputs.new_breaches > 0 }}
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      RESEND_API_KEY: ${{ secrets.RESEND_API_KEY }}
      ALERT_FROM_EMAIL: ${{ secrets.ALERT_FROM_EMAIL }}
      DASHBOARD_URL: "https://bd4l.github.io/Breaches/"
      PYTHONUNBUFFERED: "1"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install requests  # Add requests for Resend API
      - name: Send email alerts
        run: |
          echo "ðŸ“§ Processing email alerts for new breaches..."
          python scrapers/email_alerts.py --since-minutes 35
          echo ""
          echo "âœ… Email alert processing complete"
      - name: Log alert summary
        run: |
          echo "ðŸš¨ BREACH ALERT SUMMARY:"
          echo "ðŸ“Š New breaches detected: ${{ needs.summary.outputs.new_breaches }}"
          echo "ðŸ“Š Total new items: ${{ needs.summary.outputs.new_items }}"
          echo "ðŸ‘¥ Additional people affected: ${{ needs.summary.outputs.new_affected }}"
          echo ""
          echo "ðŸ”— View dashboard: https://bd4l.github.io/Breaches/"
          echo "ðŸ“§ Email alerts have been sent to subscribers"

  # ============================================================================
  # NOTIFICATION JOB - Send alerts if significant changes detected (Legacy)
  # ============================================================================

  notification:
    name: "ðŸ”” Breach Alert Notification (Legacy)"
    runs-on: ubuntu-latest
    needs: summary
    if: ${{ always() && needs.summary.outputs.new_breaches > 0 }}
    steps:
      - name: Send breach alert
        run: |
          echo "ðŸš¨ BREACH ALERT: ${{ needs.summary.outputs.new_breaches }} new breach notifications detected!"
          echo "ðŸ“Š Total new items: ${{ needs.summary.outputs.new_items }}"
          echo "ðŸ‘¥ Additional people affected: ${{ needs.summary.outputs.new_affected }}"
          echo ""
          echo "ðŸ”— View dashboard: https://hackermanmarlin.github.io/Breaches/"
          echo ""
          echo "ðŸ“§ Email alerts are now handled by the email-alerts job above."


